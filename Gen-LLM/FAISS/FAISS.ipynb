{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "\n",
    "# Copyright 2025 North Carolina State University\n",
    "\n",
    "# Authored by\n",
    "# Pranshav Gajjar, Abiodun Ganiyu, and Vijay K. Shah\n",
    "# NextG Wireless Lab, North Carolina State University\n",
    "\n",
    "############################################################# \n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "from docx import Document  # python-docx for Word processing\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "# Unzip the docs\n",
    "def unzip_docs(zip_file_path, extract_to='docs'):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "# Extract text from a PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(file_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Extract text from a Word document\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# Extract text from a Markdown file\n",
    "def extract_text_from_md(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Process documents and extract text\n",
    "def process_documents(directory):\n",
    "    documents = []\n",
    "    for file_path in Path(directory).rglob('*'):\n",
    "        if file_path.suffix.lower() == '.pdf':\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "        elif file_path.suffix.lower() == '.docx':\n",
    "            text = extract_text_from_docx(file_path)\n",
    "        elif file_path.suffix.lower() == '.md':\n",
    "            text = extract_text_from_md(file_path)\n",
    "        else:\n",
    "            continue\n",
    "        documents.append(LangchainDocument(page_content=text, metadata={\"source\": str(file_path)}))\n",
    "    return documents\n",
    "\n",
    "# Example usage\n",
    "zip_file_path = 'docs.zip'\n",
    "\n",
    "unzip_docs(zip_file_path)\n",
    "docs = process_documents('docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128,)# separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"])\n",
    "chunked_docs = splitter.split_documents(docs)\n",
    "len(chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Load existing FAISS index\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "\n",
    "# Use the new import path to avoid deprecation warning\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "db = None\n",
    "doc_counter = 0  # To track how many documents have been processed\n",
    "batch_count = 0   # To keep track of the number of batches\n",
    "\n",
    "with tqdm(total=len(chunked_docs), desc=\"Ingesting documents\") as pbar:\n",
    "    for d in chunked_docs:\n",
    "        doc_counter += 1\n",
    "        if db is not None:\n",
    "            db.add_documents([d])  # Ensure d is passed as a list\n",
    "        else:\n",
    "            db = FAISS.from_documents([d], embeddings)  # Ensure d is passed as a list\n",
    "        \n",
    "        # Save the database and reset after every 10,000 documents\n",
    "        if doc_counter % 25000 == 0:\n",
    "            batch_count += 1  # Increment the batch count\n",
    "            db.save_local(f'FAISS/final_bge_ORAN_batch_dc_{doc_counter}')  # Save with batch count\n",
    "            db = None  # Reset db to None to create a new FAISS index for the next batch\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "# Save any remaining documents at the end of the loop\n",
    "if db is not None:\n",
    "    batch_count += 1  # Increment the batch count for the last save\n",
    "    db.save_local(f'FAISS/final_bge_ORAN_batch_{batch_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Path to the FAISS folder\n",
    "faiss_folder = \"FAISS\"\n",
    "\n",
    "# List all FAISS database folders inside FAISS directory\n",
    "db_paths = [os.path.join(faiss_folder, d) for d in os.listdir(faiss_folder) if os.path.isdir(os.path.join(faiss_folder, d))]\n",
    "\n",
    "# Ensure there are FAISS databases available\n",
    "if not db_paths:\n",
    "    raise ValueError(\"No FAISS databases found in the folder.\")\n",
    "\n",
    "# Load the first FAISS vector store\n",
    "main_db = FAISS.load_local(db_paths[0], embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Merge all other vector stores into the first one\n",
    "for path in db_paths[1:]:\n",
    "    db_to_merge = FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)\n",
    "    main_db.merge_from(db_to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged vector store\n",
    "main_db.save_local(\"merged_with_metadata\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
