{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "\n",
    "# Copyright 2025 North Carolina State University\n",
    "\n",
    "# Authored by\n",
    "# Pranshav Gajjar, Abiodun Ganiyu, and Vijay K. Shah\n",
    "# NextG Wireless Lab, North Carolina State University\n",
    "\n",
    "############################################################# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "combined_faiss = FAISS.load_local(\"FAISS\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieverb=combined_faiss.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 35})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_huggingface.llms import HuggingFaceEndpoint\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "HUGGINGFACEHUB_API_TOKEN = \"\"\n",
    "\n",
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN\n",
    "\n",
    "\n",
    "\n",
    "_qa_prompt = \"\"\"\n",
    "You are an O-RAN assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question in as much detail as possible. \\\n",
    "These are the relevant documents from the official O-RAN Specifications:\n",
    "\n",
    "{context}\n",
    "\n",
    "\n",
    "The current input from the user is mentioned below:\n",
    "{input}\n",
    "\n",
    "It can happen that some documents in the provided context have incomplete information and other documets complete that.\n",
    "If you see some overlap in some steps combine that to obtain the final response.\n",
    "Try to generate a detailed response and clearly mention the response in a step by step manner.\n",
    "The steps should be numbered.\n",
    "After you have obtained all the steps and generated the response, if you find any other information that is relevant, add a section in the end called other relevant information and have an explaination of generated response and what is happening in the steps. Keep this concise.\n",
    "In the end also mention the confidence scores regarding the description that you have generated. Only give one final score at the end between 0-100, after the label Confidence Score and no ther informtation\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "qa_prompt = PromptTemplate.from_template(_qa_prompt)\n",
    "llm_model = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "\n",
    "#llm1=HuggingFaceEndpoint(repo_id=llm_model, max_new_tokens=30000, streaming=False)\n",
    "llm2=HuggingFaceEndpoint(repo_id=llm_model, max_new_tokens=4000, streaming=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the BAAI/bge-reranker-base model and tokenizer\n",
    "reranker_model_name = 'BAAI/bge-reranker-v2-m3'\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieverb = combined_faiss.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 100})\n",
    "\n",
    "def query_oran_assistant_docs(prompt: str):\n",
    "    \"\"\"\n",
    "    Takes a user query, retrieves documents, reranks them, and uses the top 10 documents\n",
    "    as context to generate a detailed answer.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user query.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The final prompt, context, top documents, top document sources, and ranked results.\n",
    "    \"\"\"\n",
    "    # Retrieve documents\n",
    "    results = retrieverb.get_relevant_documents(prompt)\n",
    "\n",
    "    # Prepare the documents for reranking\n",
    "    inputs = [(prompt, result.page_content) for result in results]\n",
    "\n",
    "    # Tokenize and encode the inputs\n",
    "    encoded_inputs = reranker_tokenizer.batch_encode_plus(\n",
    "        inputs, padding=True, truncation=True, return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Get relevance scores from the reranker model\n",
    "    with torch.no_grad():\n",
    "        outputs = reranker_model(**encoded_inputs)\n",
    "        scores = outputs.logits.squeeze(-1)  # Get relevance scores\n",
    "\n",
    "    # Rank the results based on the scores\n",
    "    ranked_results = sorted(zip(scores, results), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Extract top-ranked documents, ensuring uniqueness\n",
    "    unique_ranked_results = []\n",
    "    seen_contents = set()\n",
    "\n",
    "    for _, doc in ranked_results:\n",
    "        if doc.page_content not in seen_contents:\n",
    "            unique_ranked_results.append(doc)\n",
    "            seen_contents.add(doc.page_content)\n",
    "        if len(unique_ranked_results) == 15:  # Stop after collecting top 15 unique documents\n",
    "            break\n",
    "\n",
    "    # Prepare context from the top-ranked documents\n",
    "    top_documents = unique_ranked_results\n",
    "    context = \"\\n\\nDocument: \\n\\n\".join([doc.page_content for doc in top_documents])\n",
    "\n",
    "    # Format the final prompt\n",
    "    final_prompt = qa_prompt.format(input=prompt, context=context)\n",
    "\n",
    "    # Extract sources for only the top_documents\n",
    "    top_document_sources = [doc.metadata['source'] for doc in top_documents]\n",
    "    unique_sources = list(dict.fromkeys(top_document_sources))  # Ensure unique sources\n",
    "\n",
    "    return final_prompt, context, top_documents, unique_sources, ranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Give the UE Initial Access procedure between gNB-DU, gNB-CU, and AMF.\"\n",
    "final_prompt, context, top_documents, unique_sources, ranked_results = query_oran_assistant_docs(query)\n",
    "print(llm2(final_prompt))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
